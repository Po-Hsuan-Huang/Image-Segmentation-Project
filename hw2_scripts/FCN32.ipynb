{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 26 23:42:11 2018\n",
    "\n",
    "@author: pohsuanh\n",
    "\n",
    "\n",
    "Fully Covolutional Network FCN-32s. \n",
    "\n",
    "FCN-32s network is based on VGG-16\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "    \n",
    "def conv2d(inputs, filters, kernel_size, name, kernel_regularizer, trainable,  activation = 'relu'):\n",
    "    \n",
    "    x = tf.layers.conv2d(inputs, filters, kernel_size,\n",
    "                          activation= 'linear',\n",
    "                          padding='valid',\n",
    "                          name= name,\n",
    "                          kernel_regularizer= kernel_regularizer,\n",
    "                          trainable = trainable)\n",
    "    \n",
    "    x2 = tf.layers.batch_normalization(x)\n",
    "    \n",
    "    if activation == 'relu':\n",
    "    \n",
    "        x3 = tf.nn.relu(x2)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        \n",
    "        x3 = tf.nn.sigmoid(x2)\n",
    "    \n",
    "    return x3\n",
    "\n",
    "\n",
    "def dropout( x, rate ,  name, training = True ):\n",
    "    \n",
    "    if False :\n",
    "    \n",
    "        y = tf.compat.v2.keras.layers.SpatialDropout2D(rate = 0.4)\n",
    "        \n",
    "    else :\n",
    "    \n",
    "        y =tf.layers.dropout(x, rate = 0.4, training = training , name =name)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "    \n",
    "\n",
    "def adapt_network_for_any_size_input():\n",
    "    \n",
    "    pass\n",
    "\n",
    "class fcn_model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        with tf.variable_scope(\"down_sampling\", reuse = tf.AUTO_REUSE):\n",
    "    \n",
    "            L2 = keras.regularizers.l2(l=0.1)\n",
    "                                \n",
    "            self.trainable = True\n",
    "            \n",
    "            seed = 2019\n",
    "            \n",
    "            self.features = tf.placeholder(tf.float32,[None,500,500,3])\n",
    "            \n",
    "            self.labels = tf.placeholder(tf.float32,[None,500,500,21])\n",
    "            \n",
    "            self.loss_masks = tf.placeholder(tf.float32, self.labels.shape)\n",
    "            \n",
    "            self.c1 = conv2d(self.features, 64, (7, 7),\n",
    "                              activation='relu',\n",
    "                              name='block1_conv1',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable = self.trainable)\n",
    "            \n",
    "            self.d1 = dropout(self.c1, rate = 0.4, training = self.trainable , name ='block1_dp1')\n",
    "\n",
    "            self.c2 =  conv2d(self.d1, 64, (7, 7),\n",
    "                              activation='relu',\n",
    "                              name='block1_conv2',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "         \n",
    "            self.d2 = dropout(self.c2, rate = 0.4,  training = self.trainable , name ='block2_dp2')\n",
    "        \n",
    "            self.p1 =  tf.layers.max_pooling2d(self.d2, (2, 2), strides=(2, 2), name='block1_pool')\n",
    "            \n",
    "            # Block 2\n",
    "            self.c3 = conv2d(self.p1, 128, (5, 5),\n",
    "                              activation='relu',\n",
    "                              name='block2_conv1',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d3 =dropout(self.c3, rate = 0.4,  training = self.trainable , name ='block2_dp1')\n",
    "        \n",
    "            \n",
    "            self.c4 = conv2d(self.d3, 128, (5, 5),\n",
    "                              activation='relu',\n",
    "                              name='block2_conv2',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d4 =dropout(self.c4, rate = 0.4,  training = self.trainable , name ='block2_dp2')\n",
    "        \n",
    "        \n",
    "            self.p2 = tf.layers.max_pooling2d(self.d4,(2, 2), strides=(2,2), name='block2_pool')\n",
    "            \n",
    "            # Block 3\n",
    "            self.c5 = conv2d (self.p2, 256, (3, 3),\n",
    "                              activation='relu',\n",
    "                              name='block3_conv1',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d5 =dropout(self.c5, rate = 0.4,  training = self.trainable , name ='block3_dp1')\n",
    "        \n",
    "            self.c6 = conv2d (self.d5, 256, (3, 3),\n",
    "                              activation='relu',\n",
    "                              name='block3_conv2',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d6 =dropout(self.c6, rate = 0.4,  training = self.trainable , name ='block3_dp2')\n",
    "        \n",
    "            \n",
    "            self.c7 = conv2d (self.d6, 256, (3, 3),\n",
    "                              activation='relu',\n",
    "                              name='block3_conv3',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d7 =dropout(self.c7, rate = 0.4,  training = self.trainable , name ='block3_dp3')\n",
    "        \n",
    "            \n",
    "            self.p3 = tf.layers.max_pooling2d(self.d7, (2, 2), strides=(2, 2), name='block3_pool')\n",
    "            \n",
    "            # Block 4\n",
    "            self.c8 = conv2d (self.p3, 512, (3, 3),\n",
    "                              activation='relu',\n",
    "                              name='block4_conv1',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d8 =dropout(self.c8, rate = 0.4,  training = self.trainable , name ='block4_dp1')\n",
    "        \n",
    "            self.c9 = conv2d (self.d8, 512, (3, 3),\n",
    "                              activation='relu',\n",
    "                              name='block4_conv2',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d9 =dropout(self.c9, rate = 0.4,  training = self.trainable , name ='block4_dp2')\n",
    "        \n",
    "            self.c10 = conv2d (self.d9, 512, (3, 3),\n",
    "                              activation='relu',\n",
    "                              name='block4_conv3',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d10 =dropout(self.c10, rate = 0.4,  training = self.trainable , name ='block4_dp3')\n",
    "        \n",
    "            self.p4 = tf.layers.max_pooling2d(self.d10, (2, 2), strides=(2, 2), name='block4_pool')\n",
    "            \n",
    "            # Block 5\n",
    "            self.c11 = conv2d (self.p4, 512, (2, 2),\n",
    "                              activation='relu',\n",
    "                              name='block5_conv1',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "            \n",
    "            self.d11 =dropout(self.c11, rate = 0.4,  training = self.trainable , name ='block5_dp1')\n",
    "            \n",
    "            self.c12 = conv2d (self.d11, 512, (2, 2),\n",
    "                              activation='relu',\n",
    "                              name='block5_conv2',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "        \n",
    "            self.d12 =dropout(self.c12, rate = 0.4,  training = self.trainable , name ='block5_dp2')\n",
    "            \n",
    "            self.c13 = conv2d (self.d12, 512, (2, 2),\n",
    "                              activation='relu',\n",
    "                              name='block5_conv3',\n",
    "                              kernel_regularizer= L2,\n",
    "                              trainable  = self.trainable)\n",
    "        \n",
    "            self.d13 =dropout(self.c13, rate = 0.4,  training = self.trainable , name ='block5_dp3')\n",
    "        \n",
    "            self.p5 = tf.layers.max_pooling2d(self.d13, (2, 2), strides=(2, 2), name='block5_pool')\n",
    "            \n",
    "            # Block 6\n",
    "            \n",
    "            self.c14 = conv2d(self.p5, 4096, (7,7), \n",
    "                                 activation='relu',\n",
    "                                 name='block6_conv1',\n",
    "                                 kernel_regularizer= L2,\n",
    "                                 trainable  = self.trainable)\n",
    "            \n",
    "            self.d14 =dropout(self.c14, rate = 0.4,  training = self.trainable , name ='block6_dp1')\n",
    "        \n",
    "            self.c15 = conv2d(self.d14, 4096, (1,1),\n",
    "                                 activation='relu',\n",
    "                                 name='block6_conv2',\n",
    "                                 kernel_regularizer= L2,\n",
    "                                 trainable  = self.trainable)\n",
    "        \n",
    "            self.d15=dropout(self.c15, rate = 0.4,  training = self.trainable , name ='block6_dp2')\n",
    "            \n",
    "            self.c16 = conv2d(self.d15, 21, (1,1),\n",
    "                                 activation='relu',\n",
    "                                 name='block6_conv3',\n",
    "                                 kernel_regularizer= L2,\n",
    "                                 trainable  = self.trainable) \n",
    "        \n",
    "            self.d16 =dropout(self.c16, rate = 0.4,  training = self.trainable , name ='block6_dp3')\n",
    "\n",
    "        \n",
    "        with tf.variable_scope(\"FC32\", reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.deconv1 = tf.layers.conv2d_transpose(self.d16, 21,(16,16), strides=(4,4),\n",
    "                                           activation='sigmoid',\n",
    "                                           padding = 'same',\n",
    "                                           name='block7_deconv1',\n",
    "                                           kernel_regularizer= L2,\n",
    "                                           trainable  = self.trainable)\n",
    "            \n",
    "            self.deconv2 = tf.layers.conv2d_transpose(self.deconv1, 21, (100,100), strides=(25,25),\n",
    "                                           activation='sigmoid',\n",
    "                                           padding='same',\n",
    "                                           name='block7_deconv2',\n",
    "                                           kernel_regularizer= L2,\n",
    "                                           trainable  = self.trainable) \n",
    "        \n",
    "            self.logit1 = tf.multiply( self.deconv2, self.loss_masks, name = 'block7_mul')\n",
    "        \n",
    "        with tf.variable_scope(\"FC16\", reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            \n",
    "            self.deconv3 = tf.layers.conv2d_transpose(self.d16, 21,(16,16), strides=(5,5),\n",
    "                                           activation='sigmoid',\n",
    "                                           padding = 'same',\n",
    "                                           name='block8_deconv1',\n",
    "                                           kernel_regularizer= L2,\n",
    "                                           trainable  = self.trainable)\n",
    "            \n",
    "            self.cat1 = keras.layers.concatenate([self.deconv3, self.p4], axis = -1, name = 'block8_cat1')\n",
    "            \n",
    "            self.deconv4 = tf.layers.conv2d_transpose(self.cat1, 21, (64,64), strides=(20,20),\n",
    "                                           activation='sigmoid',\n",
    "                                           padding='same',\n",
    "                                           name='block8_deconv2',\n",
    "                                           kernel_regularizer= L2,\n",
    "                                           trainable  = self.trainable) \n",
    "        \n",
    "            self.logit2 = tf.multiply( self.deconv4, self.loss_masks, name = 'block8_mul')\n",
    "            \n",
    "        with tf.variable_scope(\"FC8\", reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.c17 = conv2d(self.d7, 128, (7,7),\n",
    "                                 activation='relu',\n",
    "                                 kernel_regularizer= L2,\n",
    "                                 trainable  = self.trainable, name = 'block9_conv1') \n",
    "            \n",
    "            self.c18 = conv2d(self.c17, 21, (7,7),\n",
    "                                 activation='relu',\n",
    "                                 kernel_regularizer= L2,\n",
    "                                 trainable  = self.trainable, name = 'block9_conv2') \n",
    "            \n",
    "            \n",
    "            self.deconv5 = tf.layers.conv2d_transpose(self.deconv3, 21,(16,16), strides=(4,4),\n",
    "                                           activation='sigmoid',\n",
    "                                           padding = 'same',\n",
    "                                           name='block8_deconv1',\n",
    "                                           kernel_regularizer= L2,\n",
    "                                           trainable  = self.trainable)\n",
    "            \n",
    "            self.cat2 = keras.layers.concatenate([self.deconv5, self.c18], axis = -1, name = 'block9_cat1')\n",
    "            \n",
    "            self.deconv6 = tf.layers.conv2d_transpose(self.cat2, 21, (20,20), strides=(5,5),\n",
    "                               activation='sigmoid',\n",
    "                               padding='same',\n",
    "                               name='block9_deconv1',\n",
    "                               kernel_regularizer= L2,\n",
    "                               trainable  = self.trainable) \n",
    "            \n",
    "        \n",
    "            self.logit3 = tf.multiply( self.deconv6, self.loss_masks, name= 'block9_mul')\n",
    "            \n",
    "    \n",
    "        with tf.variable_scope(\"logits\", reuse = tf.AUTO_REUSE):\n",
    "        \n",
    "            self.cat3 = keras.layers.concatenate([self.deconv2, self.deconv4, self.deconv2], axis = -1, name = 'block10_cat1')\n",
    "            \n",
    "            self.conv = conv2d(self.cat3, 21, (1,1),\n",
    "                                 activation='relu',\n",
    "                                 name='block10_conv1',\n",
    "                                 kernel_regularizer= L2,\n",
    "                                 trainable  = self.trainable)\n",
    "            \n",
    "            self.logit = tf.multiply( self.conv, self.loss_masks, name = 'block10_mul')\n",
    "            \n",
    "            \n",
    "        \n",
    "    def predict(self):\n",
    "\n",
    "        \n",
    "            # Do piself.xel-wise predictions :\n",
    "            \n",
    "            self.predictions = {\n",
    "                    \n",
    "              # Generate predictions (for PREDICT and EVAL mode)\n",
    "              \n",
    "              \"classes\": tf.argmax(input=tf.reshape(self.logit,(1,None)), axis=1).reshape(self.logit.shape),\n",
    "              \n",
    "              # Add `softmax_tensor` to the graph. It is used for PREDICT and by the logging_hook`.\n",
    "              \n",
    "              \"probabilities\": tf.nn.softmax(self.logit, name=\"softmax_tensor\")\n",
    "        \n",
    "              }\n",
    "            \n",
    "        \n",
    "#            if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "              \n",
    "            return self.predictions\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        # Homework requires tf.nn.sigmoid_cross_entropy_with_logits()\n",
    "        \n",
    "        self.loss_reg_L2 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        \n",
    "        self.L2_rate = 0.01\n",
    "        \n",
    "        # reduce dim from one-hot encoding to scalar encodin\n",
    "        \n",
    "    \n",
    "        self.loss_object =tf.keras.losses.SparseCategoricalCrossentropy( from_logits = True)\n",
    "                \n",
    "        self.loss = tf.reduce_mean(self.loss_object(y_true=self.labels, y_pred= self.logit)) + self.L2_rate * tf.reduce_mean(self.loss_reg_L2)\n",
    "        \n",
    "        # Configure the trainable Op (for TRAIN mode)\n",
    "        \n",
    "#        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        \n",
    "        self.train_op = self.optimizer.minimize(loss=self.loss, global_step = tf.train.get_global_step())\n",
    "        \n",
    "        return  self.train_op\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \n",
    "        # Add evaluation metrics (for EVAL mode)\n",
    "#        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        self.tp = tf.metrics.true_positives(self.labels, self.predictions['classes'])\n",
    "        \n",
    "        self.fp = tf.metrics.false_positives(self.labels, self.predictions['classes'])\n",
    "        \n",
    "        self.fn = tf.metrics.false_negatives(self.labels, self.predictions['classes'])\n",
    "        \n",
    "        self.eval_metric_ops = {\"IoU\": self.tp/(self.tp + self.fp + self.fn)}\n",
    "        \n",
    "        return self.mode, self.loss, self.eval_metric_ops\n",
    "    \n",
    "    \n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    root_dir = '/home/pohsuanh/Documents/Computer_Vision/HW6'\n",
    "\n",
    "    # Load training and eval data\n",
    "  \n",
    "    train_data, eval_data, test_data = data_load.load()\n",
    "    \n",
    "    # Construct model\n",
    "    pic = np.random.randint((test_data['x']).shape[0])\n",
    "    \n",
    "    image_sample = test_data['x'][pic]\n",
    "    \n",
    "    label_sample = test_data['y'][pic]\n",
    "    \n",
    "    image_sample = tf.Session().run(image_sample)\n",
    "    \n",
    "    label_sample = tf.Session().run(label_sample)\n",
    "    \n",
    "    plt.figure(figsize=(20,40))\n",
    "    plt.title('data')\n",
    "    plt.imshow(image_sample)\n",
    "    \n",
    "    plt.figure(figsize =(20,40))\n",
    "    plt.title('gt')\n",
    "    plt.imshow(label_sample)\n",
    "        \n",
    "    # Create the Estimator\n",
    "    \n",
    "    fcn_segmentor = tf.estimator.Estimator(\n",
    "    \n",
    "    model_fn=fcn_model_fn, model_dir=root_dir)\n",
    "   \n",
    "    # Set up logging for predictions\n",
    "\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "                                   tensors=tensors_to_log, every_n_iter=50)\n",
    "    # Train the model\n",
    "    \n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=train_data['x'],\n",
    "        y=train_data['y'],\n",
    "        batch_size=1,\n",
    "        num_epochs=None, # number of epochs to iterate over data. If None will run forever.\n",
    "        shuffle=True)\n",
    "   \n",
    "    fcn_segmentor.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=20000,\n",
    "        hooks=[logging_hook])\n",
    "   \n",
    "    # Evaluate the model and print results\n",
    "   \n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=eval_data['x'],\n",
    "        y=eval_data['y'],\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "   \n",
    "    eval_results = fcn_segmentor.evaluate(input_fn=eval_input_fn)\n",
    "   \n",
    "    print(eval_results)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
